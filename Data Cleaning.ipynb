{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from tqdm import  tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/hitesh/Learning/Downloads/Omdena AI/drive-download-20190817T143251Z-001/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/idecoNov30_ni_decoded.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns= ['tld','geo_point','fulldomain'], axis= 1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Raw JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convert Raw file back to Json file for easy extraction\"\"\"\n",
    "\n",
    "def json_file(raw_data):\n",
    "    json_acceptable_string = raw_data.replace(\"'\", r\"\\\\\")\n",
    "    return json.loads(json_acceptable_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(indict, current_key=None, outerdict=None):\n",
    "    if outerdict is None:\n",
    "        outerdict = {}\n",
    "    for key, value in indict.items():\n",
    "        newkey = current_key + '_' + key if current_key else key\n",
    "        if type(value) is not dict:\n",
    "            outerdict[newkey] = value\n",
    "        else:\n",
    "            process(value, current_key=newkey, outerdict=outerdict)\n",
    "    return outerdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_dataframe(df):\n",
    "    df_return = []   \n",
    "    for i in tqdm_notebook(range(len(df))):\n",
    "        raw_data = df.raw[i]\n",
    "        json_raw_data = json_file(raw_data)\n",
    "        processed_json= process(json_raw_data)\n",
    "        df_add = pd.DataFrame(list(processed_json.items()), columns=[\"Fields\", 'DateValue'])\n",
    "        df_return.append(df_add)\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = raw_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"      Different Approach \n",
    "\n",
    "!pip install tweet-preprocessor # This library removes URL,Mention,Hashtags,\n",
    "Reserved_words,Emojis,Smiley,Numbers from tweets\n",
    "\n",
    "import preprocessor as p\n",
    "#p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.HASHTAG,p.OPT.RESERVED,p.OPT.SMILEY,p.OPT.NUMBER) # it is possible to opt out certain processes\n",
    "p.clean('Omdena is a great platform #awesome 👍 https://github.com/s/preprocessor @wura')\n",
    "\n",
    "import preprocessor as p\n",
    "import re\n",
    "def use_tweet_preprocessor(tweet):\n",
    "    \"\"\"A python function that returns clean tweets, main functionalty based on https://github.com/s/preprocessor\n",
    "\n",
    "    Args:\n",
    "        tweet: tweet to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        cleaned tweet based on selected p.set_options and defined regrex \"\"\"\n",
    "    \n",
    "    p.set_options(p.OPT.EMOJI,p.OPT.HASHTAG,p.OPT.SMILEY) #retains emojis,hashtags and smileys in the cleaned tweets\n",
    "    clean_tweet = p.clean(tweet)\n",
    "    clean_tweet = re.sub('@[^\\s]+', '',clean_tweet) #to remove all words that starts with '@' using regrex\n",
    "    return clean_tweet\n",
    "\n",
    "ni_decoded_df['clean_content'] = ni_decoded_df['content'].apply(use_tweet_preprocessor)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def clean_tweet(input_text,pattern, replace):\n",
    "    words = re.findall(pattern, input_text)\n",
    "    for i in words:\n",
    "        input_text = re.sub(i,replace,input_text)\n",
    "    return input_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweet'] = np.vectorize(clean_tweet)(data['content'], \"@[\\w]*\",\"twitter_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweet'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis to inspect the inspect the distribution of the tweets with keywords over the time of the day and week. Also, a simple visualization to see which keywords are used very often.\n",
    "\n",
    "\"\"\"Added two more columns in the dataset (Hour and Weekday(Day Name))\n",
    "Prepared dataset with occurences of each keywords in the whole dataset and collected the ids of tweets which has the keywords in them\n",
    "Filtered out the tweets with keywords\n",
    "Came up with three visualization -- One showing the frequency of occurences of each keyword in the whole dataset -- One showing the distrbution of tweets(with signal keywords) over the time of a day -- One showing the distribution of tweets(with signal keywords) over the day in a week \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added two new columns\n",
    "data['hour'] = data['timestamp'].dt.hour\n",
    "data['day_name'] = data['timestamp'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_keywords = pd.read_csv(path + 'signal_keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converted the signal_keywords to list\n",
    "list_keywords = signal_keywords.Keyword.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to find out the occurences of each keywords in the whole dataset and collect the ids of tweets which has the keywords in them\n",
    "\n",
    "from collections import defaultdict\n",
    "tweet_dict = dict(zip(ni_decoded_df.id, ni_decoded_df.cleaned_tweet))\n",
    "dict_keyword = defaultdict(int)\n",
    "\n",
    "list_of_ids_with_keywords = []\n",
    "\n",
    "for id,tweet in tweet_dict.items():\n",
    "    has_keyword = 0\n",
    "    for word in tweet.split():\n",
    "        if word in list_keywords:\n",
    "            dict_keyword[word] += 1\n",
    "            has_keyword += 1\n",
    "  \n",
    "    if has_keyword > 0:\n",
    "        list_of_ids_with_keywords.append(id)\n",
    "\n",
    "keyword_cat_ocuurence = pd.DataFrame(list(dict_keyword.items()), columns=['Keyword', 'Number of Occurences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out only the tweets with the keywords\n",
    "df_ids = pd.DataFrame(data = list_of_ids_with_keywords, columns=['id'])\n",
    "\n",
    "df_tweets_with_keywords_only = pd.merge(ni_decoded_df, df_ids, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (11.7, 8.27)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "\n",
    "plot1 = sns.barplot(ax = ax, x='Keyword', y = 'Number of Occurences', data = keyword_cat_ocuurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing at which time of the day are tweets with keywords usually posted\n",
    "ax = sns.countplot(x=\"hour\", data=df_tweets_with_keywords_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing at which day of the weeks are usually the tweets with keywords usually posted\n",
    "\n",
    "ax = sns.countplot(x=\"day_name\", data=df_tweets_with_keywords_only)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
